\section{The Gibbs Sampler}

The Metropolis-Hastings Algorithm of the previous section can be difficult to apply when the dimension of the state space is high. 
The generation of the chain becomes too much of a multidimensional problem and becomes at least unwieldy, and possibly undoable.
Here Gibbs Sampler comes into play. The Gibbs Sampler is a spatial kind of Metropolis-Hastings Algorithm that very cleverly reduces the multidimensional problem into a sequence of \textit{one-dimensional} problem. 

Suppose a state $ \mathbf{x} $ in the state specs $ S $ is a vector in some $ m $-dimensional specs with $ \mathbf{x} = (x_1,x_2,x_3,\ldots,x_m) $. Suppose from current state $ \mathbf{x} $ we want to jump to a new state $ \mathbf{y}\in S $. According to Gibbs sampler we change coordinate one at a time,
such as $ (x_1,x_2,\ldots,x_m) \to (y_1,x_2,\ldots,x_m) \to (y_1,y_2,\ldots,x_m) \to \ldots \to (y_1,y_2,\ldots,y_m)  $, and each coordinate change is made by using the conditional distribution of that coordinate given the rest of the coordinates. 
For example, the transition $ (x_1,x_2,\ldots,x_m) \to (y_1,x_1,\ldots,x_m) $ is made by simulating from the distribution $ f(x_1|x_2,\ldots,x_m) $.
These conditional distribution of one coordinate gives all the rest are \textbf{full conditionals}. As, long as we can calculate and also simulate from all the full conditionals, a complicated multidimensional problem turns in to $ m $ one dimensional problems.

If current state $ \mathbf{x} = (x_1,x_2,x_3,\ldots,x_m)  $.Pick the coordinate to be changed at random from the $ m $ available coordinate. If the coordinate picked is $ i $, then the state $ \mathbf{y} = (x_1,x_2, \ldots , x_{j-1}, x , x_{j+1}, \ldots , x_m) $ work as a candidate state.
Then Gibbs sampler uses the Metropolis-Hastings algorithm with
\begin{align*}
    q(\mathbf{x},\mathbf{y}) &= \frac{1}{m} P( X_i = x | X_j = x_j, i\neq j )\\ 
                             &= \frac{f(\mathbf(y))}{mP(X_j = x_j, i \neq j)}
\end{align*}
Now the acceptance probability
\begin{align*}
    \alpha(\mathbf{x},\mathbf{y}) &= \min \left( \frac{f(\mathbf{y})q(\mathbf{y},\mathbf{x})}{f(\mathbf{x})q(\mathbf{x},\mathbf{y})} , 1 \right) \\ 
                                  &= \min \left(  \frac{ f(\mathbf{y})f(\mathbf{x}) }{  f(\mathbf{x})f(\mathbf{y})  } , 1  \right) \\ 
                                  &= 1
\end{align*}

Hence, Gibbs sampler is a special Metropolis-Hastings algorithm whose acceptance probability is always 1.

\subsection{Algorithm for Gibbs Sampler}
Suppose $ \mathbf{x}\in \mathds{R}^m $. Then algorithm of The Gibbs sampler can summarized as below:
\begin{enumerate}
    \item Set initial values $ \mathbf{x}^{(0)} = ( x_1^{(0)}, x_2^{(0)} , \ldots , x_m^{(0)}  ) $.
    \item Obtain a new value $ \mathbf{x}^{(j)} = ( x_1^{(j)}, x_2^{(j)}, \ldots , x_m^{(j)} )$ form $ x^{(j-1)} $ through \textit{full conditional distributions}
            \begin{align*}
                x_1^{(j)} &\sim f(x_1|x_2^{(j-1)}, \ldots, x_m^{(j-1)}  ),\\
                x_2^{(j)} &\sim f(x_2|x_1^{(j)}, x_3^{(j-1)}, \ldots, x_m^{(j-1)}), \\
                \vdots \\
                x_m^{(j)} &\sim f(x_m|x_1^{(j)}, \ldots , x_{m-1}^{(j)}  );
            \end{align*}
            \item Change counter $ j $ to $ j + 1 $ and return to step 2 until convergence is reached. 
\end{enumerate}

\subsection{Examples}
Now, we see some examples how to use Gibbs sampler.

\begin{example}[Generating standard bivariate normal distribution]

    In this example we try to generate standard bivariate normal distribution with correlation coefficient $ \rho $. Suppose,
    \[
        (X_1, X_2) \sim \text{N}\left(  
            \begin{bmatrix}
                0\\
                0
            \end{bmatrix},
            \begin{bmatrix}
                1 & \rho \\ 
                \rho & 1
            \end{bmatrix}
        \right)
    \]
    Where, $ \Sigma = \begin{bmatrix}
        1 & \rho \\ 
        \rho & 1
    \end{bmatrix} $ is known as \textbf{covariance matrix}.  Then, the probability density function of $ (X_1,X_2) $ would be,

    \[
        f(x_1,x_2) = \frac{1}{2 \pi \sqrt{1-\rho^2}} \exp \left( - \frac{1}{2(1-\rho^2)} (x^2 -2 \rho xy + y^2  ) \right)
    \]
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{images/gibbs/example1/con-plot.png}
        \caption{Contour Plot of Bivariate Normal Distribution when $ \rho = 0.5 $}
    \end{figure}

    For using Gibbs Sampler we have to calculate $ f_{X_1|X_2}(x_1|x_2) $ and $ f_{X_2|X_1}(x_2|x_1) $.

    \begin{align*}
        f_{X_1|X_2}(x_1|x_2) &= \frac{f(x_1,x_2)}{f_{X_2}(X_2)} \\ 
                    &=C_1 f(x_1,x_2) \\
                    &=C_2 \exp \left( - \frac{1}{2 (1-\rho^2)} (x_1^2 - 2 \rho x_1x_2) \right) \\
                    &= C_3 \exp \left(- \frac{1}{2 (1-\rho^2)} (x_1 - \rho x_2)^2   \right)
    \end{align*}
     Recognizing this equation as a normal density, we can conclude that, 
     \[ X_1|X_2 \sim \text{N}(\rho x_2, (1-\rho^2)) \]

    Also, 
    \begin{align*}
        f_{X_2|X_1} (x_2|x_1) &= \frac{f(x_2,x_1)}{f_{X_1}(X_1)} \\ 
                    &=C_4 f(x_2,x_1) \\
                    &=C_5 \exp \left( - \frac{1}{2 (1-\rho^2)} (x_2^2 - 2 \rho x_1x_2) \right) \\
                    &= C_6 \exp \left(- \frac{1}{2 (1-\rho^2)} (x_2 - \rho x_1)^2   \right)
    \end{align*}
    Here also we can see that,
    \[
        X_2|X_1 \sim \text{N}\left(\rho x_1, (1-\rho^2)\right)
    \]

    So, the Gibbs sampler algorithm for this case would be,
    \begin{enumerate}
        \item Set an initial state $ \left(x_1^{(0)} , x_2^{(0)} \right) $
        \item We obtain the next state $ \left( x_1^{(t+1)} , x_2^{t+1} \right) $ through the full conditional distributions, 
            \begin{align*}
                x_1^{(t+1)} &\sim \text{N}\left(\rho x_2^{(t)}, (1-\rho^2) \right) \\
                x_2 ^{(t+1)} &\sim \text{N}\left(\rho x_1^{(t+1)}, (1-\rho^2) \right) 
            \end{align*}
    \end{enumerate}
    Now choosing different initial state we study the outputs.

    \textbf{Initial State 1:} Here we are taking initial state to be $(-1,-1)$. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{images/gibbs/example1/init1.png}
        \caption{Samples when initial state $(-1,-1)$}
    \end{figure}

    \textbf{Initial State 2:} Now we consider $(0,0)$ (middle of the density) as an initial state. 

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{images/gibbs/example1/init2.png}
        \caption{Samples when initial state $(0,0)$}
    \end{figure}

    \textbf{Initial State 3:} Here we take initial state way outside form middle of distribution consider $ (-4,-4) $ as initial state. We can see from \Cref{fig:gb ex1 sample 3}, although we are taking initial state way outside, but the samples are come out to be same as above samples. 


    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{images/gibbs/example1/init3.png}
        \caption{Samples when initial state $(-4,-4)$}
        \label{fig:gb ex1 sample 3}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example1/marginal-distributions.png}
        \caption{Marginal distribution for $ X_1 $ and $ X_2 $ for different initial states}
    \end{figure}

\end{example}

\begin{example}[Simulating Beta-Binomial Distribution]

    If $ X|p \sim \text{Bin}(n,p)$ for some fix $ n $, and $ p \sim \text{Beta}(\alpha,\beta) $, then the marginal distribution of $ X $ would be called a \textbf{Beta-Binomial distribution}.
    Its probability density function,
    \begin{align*}
        f_X(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} {n\choose x} \int_{0}^{1} p^{x + \alpha -1}(1-p)^{n - x + \beta -1}dp \ \ x = 0,1,2,\ldots,n  
    \end{align*}

    For simulating Beta-Binomial distribution using Gibbs sampler we have to simulate the pair $ (X,p) $ whose joint distribution is,
    \[
       f_{(X,p)}(x,p) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} {n\choose x} p^{x + \alpha -1}(1-p)^{n - x + \beta -1} , \, x = 0,1,2,\ldots,n, \, 0<p<1.  
    \]
    
    We know form definition of Beta-Binomial distribution,
    \[
        X|p \sim \text{Bin}(n,p)
    \]

    Now,
    \begin{align*}
        f_{p|X}(p|x) &= \frac{f(x,p)}{f(x)} \\ 
                &= \frac{ \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} {n\choose x} p^{x + \alpha -1}(1-p)^{n - x + \beta -1} }{ \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} {n\choose x} \int_{0}^{1} p^{x + \alpha -1}(1-p)^{n - x + \beta -1}dp } \\
                &= \frac{ p^{x + \alpha -1}(1-p)^{n - x + \beta -1} }{ \int_{0}^{1} p^{x + \alpha -1}(1-p)^{n - x + \beta -1}dp } \\
                &= \frac{ 1 }{ \text{Beta}(x+\alpha-1,n-x+\beta-1) } p^{x + \alpha -1}(1-p)^{n - x + \beta -1}
    \end{align*}
    Hence we have,
    \begin{equation}
        \label{eq:p|x}
        f_{p|X}(p|x) = \frac{\Gamma(\alpha+\beta+n)}{\Gamma(x+\alpha)\Gamma(n-x+\beta)} p^{x + \alpha -1}(1-p)^{n - x + \beta -1}
    \end{equation}

    Form \Cref{eq:p|x} we conclude that $ p|X\sim \text{Beta}(x+\alpha,n-x+\beta) $.

    So, the Gibbs sampler algorithm for this example,
    \begin{enumerate}
        \item Choose an initial state $ p^{(0)} \sim \text{Beta}(\alpha,\beta)  $.
        \item Obtain the next state $ \left(x^{(t+1)}, p^{(t+1)}\right) $ through the full conditional distributions,
            \begin{align*}
                x^{(t+1)} &\sim \text{Bin}\left( n,p^{(t)} \right),\\
                p^{(t+1)} &\sim \text{Beta}\left(x^{(t+1)}+\alpha, n-x^{(t+1)}+\beta \right) 
            \end{align*}
        \item Replete step 2.  
    \end{enumerate}

    Now, taking $ n=10,\,\alpha=7,\,\beta=2 $ we simulate beta-binomial distribution, and we get empirical mean 7.7797, variance 3.2788 which are close to theoretical mean($ \mu $) and variance($ \sigma^2 $).
    \begin{align*}
        \mu &= \frac{n \alpha}{\alpha+\beta} = \frac{10 \times 7 }{ 7 + 2} = 7.7777 \\
        \sigma^2 &= \frac{n \alpha \beta(\alpha+\beta+n)}{(\alpha+\beta)^2(\alpha+\beta+1)} = 3.2840
    \end{align*}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{images/gibbs/example2/ex2-beta-binomial.png}
        \caption{Simulation of Beta-Binomial(10,7,2)}
        \label{fig:beta-binomial simulation}
    \end{figure}

\end{example}

\begin{example}[Finding distribution of some observed data]
    In the \Cref{eg:finding parameter} we have done a little cheating, here we did not find the mean($\mu$) in proper way, we assume the mean from the given observation, but this is not the proper way.  In this example we try to fix our mistake. 

    Now, from \textbf{Bayesian Method} we have
    \begin{equation}
        \label{eq:proper bayesian formula}
        f\left(\mu, \sigma^2|d^{n} \right) \propto f\left( d^{n} |\mu,\sigma^2 \right) f\left( \mu,\sigma^2 \right)
    \end{equation}

    The \textbf{likelihood function},
    \begin{align*}
        f\left( d^{n} |\mu,\sigma^2 \right) &= \prod_{i=1}^{n} \left( \frac{1}{\sqrt{2 \pi \sigma^2 }} \exp \left( - \frac{ (d_i - \mu)^2 }{ 2 \sigma^2 } \right) \right) \\ 
                                            &= \frac{ 1 }{ \left( 2 \pi \sigma^{2}  \right)^{\frac{n}{2}}  } \exp \left( - \frac{ \sum_{i = 1}^{n} \left( d_i - \mu \right)^{2}   }{ 2 \sigma^2 } \right) \numberthis \label{eq:likelihood function}
    \end{align*}
    
    Then \Cref{eq:proper bayesian formula} becomes, 
    \begin{equation}
         \label{eq:full equatation of proper bayesian formula}
         f(\mu,\sigma^{2} |d^{n} ) \propto \frac{ 1 }{ \left( 2 \pi \sigma^{2}  \right)^{\frac{n}{2}}  } \exp \left( - \frac{ \sum_{i = 1}^{n} \left( d_i - \mu \right)^{2}   }{ 2 \sigma^2 } \right) f(\mu,\sigma^2)
     \end{equation}

     Then, the question become how do we choose the \textbf{prior} in this case. We can consider the two possibilities,
     \begin{enumerate}
         \item We can define a joint prior distribution for $ \mu $ and $ \sigma^{2}  $, i.e. we can define $ f(\mu,\sigma^{2} ) $, but it will very hard to accomplish. 
         \item Another way we can consider $ \mu $ and $ \sigma^{2}  $ to be independent. i.e., 
             \[ f(\mu, \sigma^{2} ) = f_\mu(\mu)f_{\sigma^{2} } (\sigma^{2})  \] 
             This is relatively easy to implement.
     \end{enumerate}

     Here if we take \fdefine{conjugate prior}{In Bayesian Inference, for a given likelihood function $ f(x|\theta) $, if the posterior distribution $ f(\theta|x) $ is in the same probability density family as the prior $ f(\theta) $, then the prior and posterior are called conjugate distributions with respect to that likelihood function and the prior is called a conjugate prior for the likelihood function $ f(x|\theta) $. } as our choice of prior it will be easy to sample from posterior distribution.

     Now we have the conjugate prior for $ \mu $ for normal likelihood function is Normal distribution.
     \begin{align*}
         f_{\mu} (\mu) &\propto \tx{N}\left( \theta,\sigma_{\mu}^{2}  \right) \\
                       &\propto \frac{ 1 }{ \sqrt{2 \pi \sigma_{\mu} ^{2} } } \exp \left( - \frac{ (\mu - \theta)^{2} }{ 2 \sigma_{\mu} ^{2}  } \right)
     \end{align*}
     Where, $ \theta $ and $ \sigma_{\mu}^{2} $ are \textbf{hyperparameters} of the prior distribution. 

     And for $ \sigma^2 $ we have two choices for conjugate prior for normal likelihood function.
     \begin{enumerate}
         \item One is Inverse-gamma distribution.
             \begin{align*}
                 f_{\sigma^2}(\sigma ^{2}) &\propto \tx{Inv-Gamma}(\alpha,\beta) \\
                              &\propto \frac{ \beta^{\alpha}  }{ \Gamma(\alpha) } \left( \frac{ 1 }{ \sigma^2 } \right)^{(\alpha+1)} \exp \left( - \frac{ \beta }{ \sigma ^{2}  } \right)
             \end{align*}

             with \textbf{hyperparameters} $ \alpha > 0 $, shape\footnote{shape parameters are those parameter those effect the shape of the distribution} parameter of $ f_{\sigma ^{2}}(\sigma ^{2})  $ and $ \beta > 0 $, scale \footnote{ In family of probability distribution there is such parameter $ s $(with other parameter $ \theta $) for which the CDF satisfies $F(x;s,\theta) = F(x/s;1,\theta)$ known as scale parameter} parameter of $ f_{\sigma ^{2}}(\sigma ^{2})  $

        \item Another one is Scaled-Inverse-$\chi ^{2}$ distribution.
            \begin{align*}
                f_{\sigma ^{2} }(\sigma ^{2}) &\propto \tx{Scale-Inv-}\chi^2\left( \nu,\sigma ^{2}_0 \right)\\ 
                                              &\propto \frac{ \left( \sigma_{0}^{2}\nu/2 \right)^{\nu/2}   }{ \Gamma(\nu/2) } \frac{ \exp\left( - \frac{ \nu \sigma_0 ^{2} }{ 2 \sigma ^{2} } \right) }{ \left( \sigma^2 \right)^{1+\nu/2}  }
            \end{align*}
            With hyperparameters $ \nu $ and $ \sigma_0 ^{2} $.
     \end{enumerate}

     Now, if we choose prior of $ \sigma ^{2} $ to be Inv-Gamma$(\alpha,\beta)$ and $ \mu $ to be N$(\theta,\sigma_{\mu}^{2} )$ \Cref{eq:full equatation of proper bayesian formula} becomes,

    \begin{align*}
        f(\mu,\sigma ^{2}|d ^{n} ) \propto &\frac{ 1 }{ \left( 2 \pi \sigma^{2}  \right)^{\frac{n}{2}}  } \exp \left( - \frac{ \sum_{i = 1}^{n} \left( d_i - \mu \right)^{2}   }{ 2 \sigma^2 } \right) \times \\
        &\frac{ 1 }{ \sqrt{2 \pi \sigma_{\mu} ^{2} } } \exp \left( - \frac{ (\mu - \theta)^{2} }{ 2 \sigma_{\mu} ^{2}  } \right) \times \left( \frac{ 1 }{ \sigma^2 } \right)^{(\alpha+1)} \exp \left( - \frac{ \beta }{ \sigma ^{2}  } \right) \numberthis \label{eq:posterior imposible to calculate}
    \end{align*}

    For simulating \Cref{eq:posterior imposible to calculate} we will use Gibbs sampler. To use Gibbs sampler we have to find the full conditionals $ f(\mu|d^{n},\sigma^2 ) $ and $ f(\sigma^{2}|d^{n}, \mu) $.
    
    Now, 
    \begin{align*}
        f(\mu|d^{n},\sigma^{2} ) &\propto \exp \left( - \frac{ \sum_{i = 1}^{n}(d_i - \mu)^2  }{ 2 \sigma^{2} } \right) \times \exp \left( - \frac{ (\mu-\theta)^{2} }{ 2 \sigma_{\mu} ^{2} } \right) \\ 
                                 &\propto \exp \left( - \frac{ 1 }{ 2 } \left( \frac{ \sum_{i = 1}^{n}(d_i - \mu)^2  }{ \sigma^{2} } + \frac{ (\mu-\theta)^{2} }{ \sigma_{\mu} ^{2} } \right) \right) \\ 
                                 &\propto \exp \left( - \frac{ 1 }{ 2 } \left( \frac{ \sum_{i=1}^{n} d_i^{2} - 2 \mu \sum_{i=1}^{n} d_i + n \mu^{2} }{ \sigma^{2} } + \frac{ \mu^{2} - 2 \mu \theta + \theta^{2} }{ \sigma_\mu^{2} }  \right) \right) \\ 
                                 &\propto \exp \left( - \frac{ 1 }{ 2 } \left( \frac{ \sigma_\mu^{2} \sum_{i=1}^{n} d_i^{2} - 2 \sigma_\mu^{2} \mu \sum_{i=1}^{n} d_i + \sigma_\mu^{2} n \mu^{2} + \sigma^{2}\mu^{2} - 2 \sigma^{2}\mu \theta + \sigma^{2}\theta^{2} }{ \sigma^{2}\sigma_\mu^{2} } \right) \right) \\
                                 &\propto \exp \left( - \frac{ 1 }{ 2 }  \frac{ \left( n \sigma_{\mu}^{2} + \sigma^{2}  \right)\mu^{2} - 2 \mu \left( \sigma_{\mu}^{2}\sum_{i=1}^{n}d_i + \sigma^{2}\theta  \right)   }{ \sigma^{2}\sigma_{\mu} ^{2} } \right) \\
                                 &\propto \exp \left( - \frac{ 1 }{ 2 } \frac{ \mu^{2} - 2 \mu \left( \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + \sigma^{2}\theta  }{ n \sigma_{\mu}^{2} + \sigma^{2}  } \right) }{ \frac{ \sigma_{\mu}^{2}\sigma^{2} }{  n \sigma_{\mu}^{2} + \sigma^{2} } }  \right) \\
                                 &\propto \exp \left( - \frac{ \left( \mu - \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + \sigma^{2}\theta }{ n \sigma_{\mu}^{2}+\sigma^{2}  } \right)^{2} }{ 2 \left( \frac{ \sigma^{2}\sigma_{\mu}^{2}  }{  n \sigma_{\mu}^{2}+\sigma^{2} } \right) } \right) \\ 
                                 &\propto \tx{N}\left( \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + \sigma^{2}\theta  }{ n \sigma_{\mu}^{2} + \sigma^{2} }, \frac{ \sigma_{\mu}^{2}\sigma^{2}  }{ n \sigma_{\mu}^{2} + \sigma^{2} } \right)
    \end{align*}
    So, posterior of $ \mu $ is a normal distribution with,
    \begin{align*}
        \tx{mean }= \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + \sigma^{2}\theta  }{ n \sigma_{\mu}^{2} + \sigma^{2} }\, \tx{ and  variance }= \frac{ \sigma_{\mu}^{2}\sigma^{2}  }{ n \sigma_{\mu}^{2} + \sigma^{2} }
    \end{align*}

    Now if we take Inv-Gamma as prior of $ \sigma^{2} $ we get,
    \begin{align*}
        f(\sigma^{2}|d^{n} \mu) &\propto \left(\frac{ 1 }{\sigma^{2}}\right)^{\frac{ n }{2}} \exp \left( - \frac{ \sum_{i=1}^{n} (d_i - \mu)^{2} }{ 2 \sigma^{2} } \right) 
        \left( \frac{ 1 }{ \sigma^{2} }\right)^{\alpha+1}  \exp \left( - \frac{ \beta }{ \sigma^{2}  } \right) \\ 
                                &\propto \left( \frac{ 1 }{ \sigma^{2} } \right)^{\alpha + \frac{ n }{ 2 } + 1} 
                                \exp \left( - \frac{ 2\beta + \sum_{i=1}^{n} (d_i-\mu)^{2} }{ 2\sigma^{2} } \right) \\ 
                                &\propto \left( \frac{ 1 }{ \sigma^{2} } \right)^{\alpha + \frac{ n }{ 2 } + 1}
                                \exp \left( - \frac{ \beta + \frac{ \sum_{i=1}^{n} (d_i-\mu)^{2} }{ 2 } }{ \sigma^{2} } \right) \\ 
                                &\propto \tx{Inv-Gamma}\left( \alpha + \frac{ n }{ 2 }, \beta + \frac{ \sum_{i=1}^{n} (d_i-\mu)^{2} }{ 2 } \right)
    \end{align*}

    And now if we take Scale-Inv-$ \chi^{2} $ as prior of $ \sigma^{2} $ we the posterior to be,
    \begin{align*}
        f(\sigma^{2}|d^{n} ,\mu) &\propto \left( \frac{ 1 }{ 2 \pi \sigma^{2} } \right)^{n/2} 
        \exp \left( - \frac{ \sum_{i=1}^{n} (d_i - \mu)^{2} }{ 2 \sigma^{2} } \right) 
        \left( \frac{ 1 }{ \sigma^{2} } \right)^{1+\frac{ \nu }{ 2 }}
        \exp \left( - \frac{ \nu \sigma_0^{2} }{ 2 \sigma^{2} } \right) \\ 
                                 &\propto \left( \frac{ 1 }{ \sigma^{2} } \right)^{1+\frac{ \nu+ n }{ 2 }} 
        \exp \left( - \frac{ \sum_{i=1}^{n}(d_i-\mu)^{2} + \nu \sigma_0^{2}  }{ 2 \sigma^{2} } \right) \\ 
                                 &\propto \left( \frac{ 1 }{ \sigma^{2} } \right)^{1+\frac{ \nu+ n }{ 2 }} 
                                 \exp \left( - \frac{ (\nu + n) \frac{ \sum_{i=1}^{n}(d_i-\mu)^{2} + \nu \sigma_0^{2} }{ \nu + n } }{ 2 \sigma^{2} } \right) \\ 
                                 &\propto \tx{Scale-Inv-}\chi^{2}\left( \nu + n, \frac{ \sum_{i=1}^{n}(d_i-\mu)^{2} + \nu \sigma_0^{2} }{ \nu + n } \right)
    \end{align*}
    
    Then, the Gibbs sampler algorithm can be summarized as follows,

    When we take Inv-Gamma$(\alpha,\beta)$ as prior of $ \sigma^{2} $,
    \begin{enumerate}
        \item Set an initial state $ \left( \mu^{(0)}, (\sigma^{2})^{(0)}\right)$. Were,
            \begin{align*}
                \mu^{(0)} &\sim \tx{N}(\theta,\sigma_{\mu}^{2}  ) \\ 
                \left(\sigma^{2}\right)^{(0)} &\sim \tx{Inv-Gamma}(\alpha,\beta)  
            \end{align*}
        \item We obtain the next state $ \left( \mu^{(t+1)}, (\sigma^{2})^{(t+1)} \right) $ by the full conditionals,
            \begin{align*}
                \mu^{(t+1)} &\sim \tx{N}\left( \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + (\sigma^{2})^{(t)} \theta  }{ n \sigma_{\mu}^{2} + (\sigma^{2})^{(t)}  }, \frac{ \sigma_{\mu}^{2}(\sigma^{2})^{(t)}   }{ n \sigma_{\mu}^{2} + (\sigma^{2})^{(t)}  } \right) \\ 
                (\sigma^{2})^{(t+1)} &\sim \tx{Inv-Gamma}\left( \alpha + \frac{ n }{ 2 }, \beta + \frac{ \sum_{i=1}^{n} (d_i-(\mu)^{(t+1)} )^{2} }{ 2 } \right)
            \end{align*}
            \item Repeat step 2.
    \end{enumerate}
    Now, we know if $ X\sim \tx{Inv-Gamma}(\alpha,\beta) $ then $ \frac{ 1 }{ X } \sim \tx{Gamma}(\alpha,\beta)$. Therefore sampling from Inverse-gamma not not different.

    %% images
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example3/inv-gamma-pre-burnin.png}
        \caption{Accepted values of $ \mu $ and $ \sigma^{2} $ when prior of $ \sigma^{2} $ is Inv-Gamma$(\alpha,\beta)$}
        \label{fig:inv-gamma pre burn-in}
    \end{figure}
    Now jugging form \Cref{fig:inv-gamma pre burn-in} we have to remove first few samples as Burn-In. So after removing burn-in we get $ \mu = 9.9230 $ and $ \sigma^{2}=9.0160 $ which is close to original.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example3/inv-gamma-post-burnin.png}
        \caption{Accepted values of $ \mu $ and $ \sigma^{2} $ after removing Burn-In when prior of $ \sigma^{2} $ is Inv-Gamma$(\alpha,\beta)$}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{images/gibbs/example3/inv-gamma-scatter-post-burnin.png}
        \caption{Scatter plot of $ \mu $ and $ \sigma^{2} $ together after removing Burn-In when prior of $ \sigma^{2} $ is Inv-Gamma$(\alpha,\beta)$}
    \end{figure}
    
    Now if we take Scale-Inv-$\chi^{2}(\nu,\sigma^{2}_0)$ as prior of $ \sigma^{2} $, the algorithm become,
    \begin{enumerate}
        \item Set an initial state $ \left( \mu^{(0)}, (\sigma^{2})^{(0)}\right)$. Were,
            \begin{align*}
                \mu^{(0)} &\sim \tx{N}(\theta,\sigma_{\mu}^{2}  ), \\ 
                \left(\sigma^{2}\right)^{(0)} &\sim \tx{Scale-Inv-}\chi^{2}(\nu,\sigma^{2}_0)  
            \end{align*}
        \item We obtain the next state $ \left( \mu^{(t+1)}, (\sigma^{2})^{(t+1)} \right) $ by the full conditionals,
            \begin{align*}
                \mu^{(t+1)} &\sim \tx{N}\left( \frac{ \sigma_{\mu}^{2}\sum_{i=1}^{n} d_i + (\sigma^{2})^{(t)} \theta  }{ n \sigma_{\mu}^{2} + (\sigma^{2})^{(t)}  }, \frac{ \sigma_{\mu}^{2}(\sigma^{2})^{(t)}   }{ n \sigma_{\mu}^{2} + (\sigma^{2})^{(t)}  } \right), \\ 
                (\sigma^{2})^{(t+1)} &\sim \tx{Scale-Inv-}\chi^{2} \left( \nu+ n , \frac{ \sum_{i=1}^{n}(d_i-\mu^{(t+1)} )^{2} + \nu \sigma_0^{2} }{ \nu + n } \right)
            \end{align*}
            \item Repeat step 2.
    \end{enumerate}
    We know,
    \begin{align*}
        X &\sim \tx{Scale-Inv-}\chi^{2}\left( \nu, \sigma_0^{2} \right)\\ 
        \frac{ X }{ \nu \sigma_0^{2} } &\sim \tx{Inv-}\chi^{2}(\nu) \\ 
        \frac{ \nu \sigma_0^{2} }{ X } &\sim \chi^{2}(\nu) \\ 
        \frac{ 1 }{ X } &\sim \frac{ 1 }{ \nu \sigma_0^{2} }\chi^{2}(\nu)
    \end{align*}
    So simulating Scale-Inv-$\chi^{2} $ is also easy.

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example3/chi2-pre-burnin.png}
        \caption{Accepted values of $ \mu $ and $ \sigma^{2} $ when prior of $ \sigma^{2} $ is Scale-Inv-$\chi^{2}(\nu,\sigma^{2}_0)$}
        \label{fig:scale-inv-chi2 pre burn-in}
    \end{figure}

    Here also we remove first 25\% of term as Burn-In and then we get $ \mu $ to be 9.9230 and $ \sigma^{2} $ to be 9.0258.  
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example3/chi2-post-burnin.png}
        \caption{Accepted values of $ \mu $ and $ \sigma^{2} $ after removing Burn-In when prior of $ \sigma^{2} $ is Scale-Inv-$\chi^{2}(\nu,\sigma^{2}_0)$}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{images/gibbs/example3/chi2-scatter-post-burnin.png}
        \caption{Scatter plot of $ \mu $ and $ \sigma^{2} $ together after removing Burn-In when prior of $ \sigma^{2} $ is Scale-Inv-$\chi^{2}(\nu,\sigma^{2}_0)$}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/gibbs/example3/comparisum.png}
        \caption{Comparison between original population and simulated population with different prior}
    \end{figure}

 \end{example}


